<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>machine-learning</title>
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="org-default.css" />
</head>
<body>
<div id="content" class="container content">
<nav class="navbar is-black" id="orge9f8aef">
<div class="navbar-brand" id="orge1048d5">
<div class="navbar-item" id="orga65ed93">
<p>
<a href="index.html#ID-3a34d6d7-6f37-4573-b20e-3c93894e54ac">üè†</a>
</p>

</div>

</div>
<div class="navbar-menu" id="orgf8c0f3c">
<div class="navbar-start" id="org6cae7af">
<div class="navbar-item" id="orga16a72b">
<p>
<a href="site_map.html#ID-30ea5e38-9b41-4bcd-8631-76821e93e294">üó∫</a>
</p>

</div>

</div>
<div class="navbar-end" id="org5500b98">

</div>

</div>
</nav>
<div id="outline-container-orgf08ac45" class="outline-2">
<h2 id="orgf08ac45">Machine learning</h2>
<div class="outline-text-2" id="text-orgf08ac45">
</div>
<div id="outline-container-org8279777" class="outline-3">
<h3 id="org8279777">notes</h3>
<div class="outline-text-3" id="text-org8279777">
<ul class="org-ul">
<li>huggingface is a hub for models with api libs using pytorch/tensorflow/jax
<ul class="org-ul">
<li>spaces to run in notebook like without colab using gradio</li>
</ul></li>
<li>collab gives free 12hrs of compute</li>
<li>Forward Forward is alternative back propagation</li>
<li>wasi-nn for evaluating ml models in wasm via SIMD extensions with either wasmedge (pytorch) or wasmtime(openvino)</li>
<li><a href="https://github.com/microsoft/onnxruntime">ONNX</a> for browser evaluation of pytorch models</li>
<li>usually models can be quantized (compressed) by changing the float tensor values from fp16 to int8 with little loss
<ul class="org-ul">
<li>4bit with GPTQ compression</li>
</ul></li>
<li>LoRA is a finetunning mechanism</li>
<li>multi modal models (ViTS) combine visual image and text</li>
<li>steering vectors cache activation layer results and swap them for alignment at the same layer</li>
<li>transformers context length prediction can be expanded from the size used in training by using <a href="https://arxiv.org/pdf/2108.12409.pdf">ALiBi in MPT</a> (or 0.5 scale on the positional embeddings like in SUPERHOT?)
<ul class="org-ul">
<li><a href="https://github.com/salesforce/xGen">xGen 8k context</a></li>
<li><a href="https://huggingface.co/lmsys/longchat-13b-16k">longchat 16k context</a></li>
<li><a href="https://huggingface.co/syzymon/long_llama_3b">long llama 256k</a></li>
</ul></li>
<li><a href="https://github.com/EleutherAI/lm-evaluation-harness">https://github.com/EleutherAI/lm-evaluation-harness</a></li>
</ul>
</div>
</div>
<div id="outline-container-org563d9c4" class="outline-3">
<h3 id="org563d9c4">image generation</h3>
<div class="outline-text-3" id="text-org563d9c4">
<ul class="org-ul">
<li>dalle mini</li>
<li><a href="https://github.com/hlky/stable-diffusion">stable diffusion</a>
<ul class="org-ul">
<li><code>stable-diffusion-webui/webui.sh --listen --no-half --use-cpu all</code> for cpu only inside container
<ul class="org-ul">
<li><code>podman run --security-opt label=type:nvidia_container_t -p 7860:7860 -v /home/jam/git/stable-diffusion-webui/:/tmp/stable-diffusion-webui:Z -v /home/jam/.local:/.local:Z -v /home/jam/.cache:/.cache:Z -v /home/jam/.config:/.config --userns keep-id --rm -it jam/cuda:1 /bin/bash # COMMANDLINE_ARGS="--listen --no-half --use-cpu all --xformers --no-half-vae --opt-sdp-attention" /tmp/stable-diffusion-webui/webui.sh</code></li>
</ul></li>
<li><a href="https://github.com/andreasjansson/cog-stable-diffusion/tree/animate">animation with interpolation</a></li>
<li><a href="https://github.com/carson-katri/dream-textures">dreambooth plugin for blender textures</a></li>
<li><a href="https://github.com/riffusion/riffusion-app">Generate music from spectrograph</a></li>
<li><a href="https://github.com/lllyasviel/ControlNet">Controlnet</a> guided Stable diffusion from scribbles/images/depth maps</li>
<li><a href="https://github.com/facebookresearch/segment-anything">inpainting selection with Segment Anything Model</a></li>
<li>fine tune with <a href="https://github.com/cloneofsimo/lora">lora</a> and <a href="https://github.com/huggingface/diffusers/tree/main/examples/dreambooth">dreambooth</a></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org6d31a4e" class="outline-3">
<h3 id="org6d31a4e">Large Language Models</h3>
<div class="outline-text-3" id="text-org6d31a4e">
<ul class="org-ul">
<li>chinchilla paper showed most models are over parameterized without enough data</li>
<li>llama 2
<ul class="org-ul">
<li>4096 context length (7B, 13B, 70B)</li>
<li>2 trillion tokens (~8% programming data, tokenizer replaces spaces with underscores)</li>
</ul></li>
<li><a href="https://github.com/facebookresearch/llama">Llama 1</a> (chinchilla optimal) recreated 3B and 7B as <a href="https://huggingface.co/togethercomputer/RedPajama-INCITE-Chat-3B-v1">RedPajama</a> (gpt-neox tokenizer) and <a href="https://huggingface.co/openlm-research/open_llama_7b">OpenLLaMa</a> on 1T tokens
<ul class="org-ul">
<li>llama tokenizer does not make multiple whitespace significant (thus cant code in python) unlike GPT-NEOX</li>
<li>context length of 2048</li>
<li>weights unpublished
<ul class="org-ul">
<li>torrent <code>magnet:?xt=urn:btih:ZXXDAUWYLRUXXBHUYEMS6Q5CE5WA3LVA&amp;dn=LLaMA</code> (hugginface has several copies too)</li>
</ul></li>
<li>more than 1,000,000 hours of 40GiB GPU (400 watts) compute hours for 65B model
<ul class="org-ul">
<li>1000 tons of CO2 (2.6 million KWh hours)</li>
<li><a href="https://github.com/ggerganov/llama.cpp">ggml is 4 bit and cpu</a> (adding more gpu and 3/5/6 bit quant)
<ul class="org-ul">
<li>set params to <code>top_k=40</code>, <code>temperature=0.7</code>, <code>top_p=0</code> and a repeat penalty <code>repeat_last_n=64</code> and <code>repeat_penalty=1.3</code></li>
</ul></li>
</ul></li>
<li><a href="https://github.com/antimatter15/alpaca.cpp">Alpaca</a> is <a href="https://github.com/tatsu-lab/stanford_alpaca">refined by standford</a> for chat instructions
<ul class="org-ul">
<li><a href="https://github.com/lm-sys/FastChat">Vicuna</a> is refined alpaca with sharegpt with 'conversation format'</li>
</ul></li>
<li>WizardLM is fine tuned with  'Evol-Instruct' (llm generated) data for 'deep knowledge'
<ul class="org-ul">
<li>VicunaWizard combines the Vicuna and Wizardlm</li>
</ul></li>
<li>Orca is Supervised Fine Tuned GPT4 output in alpaca format (top of llm leaderboard with llama2)</li>
<li>Can be fine tuned with <a href="https://github.com/tloen/alpaca-lora">LoRA</a></li>
</ul></li>
<li>Flan-T5 for text summary</li>
<li>Donut for document image question answering</li>
<li>Vilt for image question answering</li>
</ul>
</div>
<div id="outline-container-org6ca6df5" class="outline-4">
<h4 id="org6ca6df5">Codegen</h4>
<div class="outline-text-4" id="text-org6ca6df5">
<ul class="org-ul">
<li><a href="https://github.com/bigcode-project/starcoder">StarCoder</a></li>
<li><a href="https://github.com/salesforce/CodeT5">CodeT5+</a></li>
<li><a href="https://github.com/moyix/fauxpilot">Fauxpilot</a>
<ul class="org-ul">
<li>uses salesforce/Codegen which supports natural language input and generation of C, C++, Go, Java, JavaScript, Python (BIG QUERY).
<ul class="org-ul">
<li>specialized in python with the BIGPYTHON dataset</li>
</ul></li>
<li>Converts salesforce/codegen model into GPTJ</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org4789af2" class="outline-3">
<h3 id="org4789af2">Speech to text</h3>
<div class="outline-text-3" id="text-org4789af2">
<ul class="org-ul">
<li>open ai whisper translation</li>
</ul>
<div class="org-src-container">
<pre class="src src-sh" id="org0468840">pip install --user git+https://github.com/openai/whisper.git
pip install --user yt-dlp
VID="TyvE8oexEAA"
yt-dlp https://www.youtube.com/watch?v=${VID} --format m4a -o "%(id)s.%(ext)s"
whisper "/content/${VID}.m4a" --model small --language English
</pre>
</div>
</div>
</div>
<div id="outline-container-org72975d4" class="outline-3">
<h3 id="org72975d4">Text to speech</h3>
<div class="outline-text-3" id="text-org72975d4">
<ul class="org-ul">
<li><a href="https://github.com/neonbjb/tortoise-tts">tortoise-tts</a> based on dalle</li>
<li><a href="https://github.com/coqui-ai/TTS">coqui-ai</a> with YourTTS/FreeVC voice cloning
<ul class="org-ul">
<li><a href="https://github.com/coqui-ai/TTS/blob/dev/TTS/tts/utils/text/cleaners.py">english cleaner</a> for abbrevations/dates/times/numbers</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org018b421" class="outline-3">
<h3 id="org018b421">Agent prompts</h3>
<div class="outline-text-3" id="text-org018b421">
<div class="org-src-container">
<pre class="src src-text" id="org0d16f21">Answer the following questions as best you can. You have access to the following tools:

{tools}

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Begin! Remember the following: {context}

Question: {input}
{agent_scratchpad}
</pre>
</div>
</div>
</div>
<div id="outline-container-orgac74dec" class="outline-3">
<h3 id="orgac74dec">transformers examples</h3>
<div class="outline-text-3" id="text-orgac74dec">
<ul class="org-ul">
<li>can set accelerate device with cli <code>accelerate launch --cpu main.py</code>, env <code>ACCELERATE_USE_CPU=True</code> or python <code>accelerator = Accelerator(cpu=True)</code></li>
</ul>
<div class="org-src-container">
<pre class="src src-python" id="orgd0e35d4">#!/usr/bin/env python3
# sentiment analysis
from transformers import pipeline
# from transformers import load_dataset
classifier = pipeline("sentiment-analysis")
print(classifier("ara ara"))

# LLM
import torch
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM
MIN_TRANSFORMERS_VERSION = '4.25.1'
print("checking transformers version")
# check transformers version
assert transformers.__version__ &gt;= MIN_TRANSFORMERS_VERSION, f'Please upgrade transformers to version {MIN_TRANSFORMERS_VERSION} or higher.'
print("Getting tokenizer")
# init
tokenizer = AutoTokenizer.from_pretrained("togethercomputer/RedPajama-INCITE-Chat-3B-v1")
print("getting model")
model = AutoModelForCausalLM.from_pretrained("togethercomputer/RedPajama-INCITE-Chat-3B-v1", torch_dtype=torch.bfloat16) # , device_map='auto', load_in_8bit=True
# infern
print("Feeding prompt")
prompt = "&lt;human&gt;: Where is Jimmy Hoffa?\n&lt;bot&gt;:"
inputs = tokenizer(prompt, return_tensors='pt').to(model.device)
input_length = inputs.input_ids.shape[1]
print("Generating")
outputs = model.generate(
    **inputs, max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.7, top_k=50, return_dict_in_generate=True
)
token = outputs.sequences[0, input_length:]
output_str = tokenizer.decode(token)
print(output_str)

# Agents
import torch
from transformers import LocalAgent
model = "bigcode/tiny_starcoder_py"
agent = LocalAgent.from_pretrained(model, torch_dtype=torch.bfloat16)
text = "Sally sold sea shells down by the seashore."
prompt = "Summarize the text given in the variable `text` and read it out loud."
agent.run(prompt, text=text)#return_code=True
#https://huggingface.co/datasets/huggingface-tools/default-prompts
# quant with offload
model = AutoModelForCausalLm.from_pretrained("bigcode/starcoder", device_map="auto", load_in_8bit=True, offload_folder="offload", offload_state_dict=True)
# distribute weights on cpu/gpu
from accelerate import infer_auto_device_map
from accelerate import init_empty_weights
from transformers import GPTBigCodeConfig, GPTBigCodeForCausalLM

device_map = {}
model_config = GPTBigCodeConfig()
with init_empty_weights(): # get device_map without loading model weights
    model = GPTBigCodeForCausalLM(model_config)
    device_map = infer_auto_device_map(model, max_memory={0: "0GiB", "cpu": "24GiB"})

# Tools for load_tool: 
#"document-question-answering"
#"image-captioning"
#"image-question-answering"
#"image-segmentation"
#"speech-to-text"
#"summarization"
#"text-classification"
#"text-question-answering"
#"text-to-speech"
#"translation"
#
# Extra tools from hub
#"text-to-image"
from transformers import load_tool

text = "Sally sold sea shells down by the seashore. She was trying to pay off her student loans. She is homeless and hungry. She owes the IRS too."
summarizer = load_tool("summarization")
summarized_text = summarizer(text)
print(f"Summary: {summarized_text}")

text = "Sally sold sea shells down by the seashore. She was trying to pay off her student loans. She is homeless and hungry. She owes the IRS too."
question = "What is being sold?"
text_qa = load_tool("text-question-answering")
answer = text_qa(text=text, question=question)
print(f"The answer is {answer}.")

from PIL import Image
image = Image.open("dog.png")#.resize((256, 256))# 384 - 640 px on Vilt images
question = "What color is the dog?"
image_qa = load_tool("image-question-answering")
answer = image_qa(image=image, question=question)
print(f"The answer is {answer}")

# document is a png of pdf
from pdf2image import convert_from_path
#import os
#os.environ["PROTOCOL_BUFFERS_PYTHON"] = "python"
images = convert_from_path('./bitcoin.pdf')
question = "What the document about?"
document = images[0]
document_qa = load_tool("document-question-answering", device="cpu")
answer = document_qa(document, question=question)
print(f"The answer is {answer}.")

import torch
image_generator = load_tool("huggingface-tools/text-to-image")
image_generator.device = torch.device("cpu")
#image_generator.default_checkpoint = "runwayml/stable-diffusion-v1-5"
image_generator.setup()
image_generator.pipeline.to("cpu")
image_generator.pipeline.enable_attention_slicing()
image_generator.pipeline.enable_sequential_cpu_offload()

prompt = "Dog, noble, majestic, realistic, high definition, pitbull"
image = image_generator(prompt=prompt)
image.save("test.png")

</pre>
</div>
</div>
</div>
</div>
</div>
</body>
</html>