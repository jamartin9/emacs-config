:PROPERTIES:
:ID:       a7f0c442-c2ec-4d7e-bc65-628631138d81
:END:
#+title: python
#+OPTIONS: toc:nil num:nil date:nil \n:nil html-style:nil author:nil timestamp:nil title:nil html-postamble:nil html5-fancy:t
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="org-default.css" />
#+HTML_CONTENT_CLASS: container content
#+HTML_DOCTYPE: html5
#+INCLUDE: "css.org::navbar" :only-contents t
* python
- override behavior with double underscore methods like ~__str(self)__~
- create and activate virtual environment ~python -m venv venv && source venv/bin/activate~
- run a http server on port 8080 ~python -m http.server 8080~
- pretty print json ~python -m json.tool~
- [[https://peps.python.org/pep-0703/][PEP to remove GIL]] but most likely going to go with [[https://peps.python.org/pep-0554/][sub interpreters PEP]]
- typing is optional ex. ~def test(name: str) -> str:~
- functools provides decorator annotations for things like ~@cache~
- decorators in stdlib such as ~@staticmethod~
- ~@dataclass(frozen=True)~ for immutable objects
** transformers examples
#+NAME: hugginface.py
#+BEGIN_SRC python :tangle no
#!/usr/bin/env python3
# sentiment analysis
from transformers import pipeline
# from transformers import load_dataset
classifier = pipeline("sentiment-analysis")
print(classifier("ara ara"))

# LLM
import torch
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM
MIN_TRANSFORMERS_VERSION = '4.25.1'
print("checking transformers version")
# check transformers version
assert transformers.__version__ >= MIN_TRANSFORMERS_VERSION, f'Please upgrade transformers to version {MIN_TRANSFORMERS_VERSION} or higher.'
print("Getting tokenizer")
# init
tokenizer = AutoTokenizer.from_pretrained("togethercomputer/RedPajama-INCITE-Chat-3B-v1")
print("getting model")
model = AutoModelForCausalLM.from_pretrained("togethercomputer/RedPajama-INCITE-Chat-3B-v1", torch_dtype=torch.bfloat16) # , device_map='auto', load_in_8bit=True
# infern
print("Feeding prompt")
prompt = "<human>: Where is Jimmy Hoffa?\n<bot>:"
inputs = tokenizer(prompt, return_tensors='pt').to(model.device)
input_length = inputs.input_ids.shape[1]
print("Generating")
outputs = model.generate(
    **inputs, max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.7, top_k=50, return_dict_in_generate=True
)
token = outputs.sequences[0, input_length:]
output_str = tokenizer.decode(token)
print(output_str)
#+END_SRC
