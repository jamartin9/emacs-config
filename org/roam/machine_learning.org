:PROPERTIES:
:ID:       187181c4-598e-4373-96e0-b66b5c676d23
:END:
#+title: machine-learning
#+OPTIONS: toc:nil num:nil date:nil \n:nil html-style:nil author:nil timestamp:nil title:nil html-postamble:nil html5-fancy:t
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="org-default.css" />
#+HTML_CONTENT_CLASS: container content
#+HTML_DOCTYPE: html5
#+INCLUDE: "css.org::navbar" :only-contents t
* Machine learning
** notes
- huggingface is a hub for models with api libs using pytorch/tensorflow/jax
  - spaces to run in notebook like without colab using gradio
- collab gives free 12hrs of compute
- Forward Forward is alternative back propagation
- wasi-nn for evaluating ml models in wasm via SIMD extensions with either wasmedge (pytorch) or wasmtime(openvino)
- [[https://github.com/microsoft/onnxruntime][ONNX]] for browser evaluation of pytorch models
- usually models can be quantized (compressed) by changing the float tensor values from fp16 to int8 with little loss
  - 4bit with GPTQ compression
- LoRA is a finetunning mechanism
- multi modal models (ViTS) combine visual image and text
- steering vectors cache activation layer results and swap them for alignment at the same layer
- transformers context length prediction can be expanded from the size used in training by using [[https://arxiv.org/pdf/2108.12409.pdf][ALiBi in MPT]] (or 0.5 scale on the positional embeddings like in SUPERHOT?)
  - [[https://github.com/salesforce/xGen][xGen 8k context]]
  - [[https://huggingface.co/lmsys/longchat-13b-16k][longchat 16k context]]
  - [[https://huggingface.co/syzymon/long_llama_3b][long llama 256k]]
- https://github.com/EleutherAI/lm-evaluation-harness
** image generation
  - dalle mini
  - [[https://github.com/hlky/stable-diffusion][stable diffusion]]
    - ~stable-diffusion-webui/webui.sh --listen --no-half --use-cpu all~ for cpu only inside container
      - ~podman run --security-opt label=type:nvidia_container_t -p 7860:7860 -v /home/jam/git/stable-diffusion-webui/:/tmp/stable-diffusion-webui:Z -v /home/jam/.local:/.local:Z -v /home/jam/.cache:/.cache:Z -v /home/jam/.config:/.config --userns keep-id --rm -it jam/cuda:1 /bin/bash # COMMANDLINE_ARGS="--listen --no-half --use-cpu all --xformers --no-half-vae --opt-sdp-attention" /tmp/stable-diffusion-webui/webui.sh~
    - [[https://github.com/andreasjansson/cog-stable-diffusion/tree/animate][animation with interpolation]]
    - [[https://github.com/carson-katri/dream-textures][dreambooth plugin for blender textures]]
    - [[https://github.com/riffusion/riffusion-app][Generate music from spectrograph]]
    - [[https://github.com/lllyasviel/ControlNet][Controlnet]] guided Stable diffusion from scribbles/images/depth maps
    - [[https://github.com/facebookresearch/segment-anything][inpainting selection with Segment Anything Model]]
    - fine tune with [[https://github.com/cloneofsimo/lora][lora]] and [[https://github.com/huggingface/diffusers/tree/main/examples/dreambooth][dreambooth]]
** Large Language Models
  - chinchilla paper showed most models are over parameterized without enough data
  - [[https://github.com/facebookresearch/llama][Llama]] (chinchilla optimal) recreated 3B and 7B as [[https://huggingface.co/togethercomputer/RedPajama-INCITE-Chat-3B-v1][RedPajama]] (gpt-neox tokenizer) and [[https://huggingface.co/openlm-research/open_llama_7b][OpenLLaMa]] on 1T tokens
    - llama tokenizer does not make multiple whitespace significant (thus cant code in python) unlike GPT-NEOX
    - context length of 2048
    - weights unpublished
      - torrent ~magnet:?xt=urn:btih:ZXXDAUWYLRUXXBHUYEMS6Q5CE5WA3LVA&dn=LLaMA~ (hugginface has several copies too)
    - more than 1,000,000 hours of 40GiB GPU (400 watts) compute hours for 65B model
      - 1000 tons of CO2 (2.6 million KWh hours)
      - [[https://github.com/ggerganov/llama.cpp][ggml is 4 bit and cpu]] (adding more gpu and 3/5/6 bit quant)
        - set params to ~top_k=40~, ~temperature=0.7~, ~top_p=0~ and a repeat penalty ~repeat_last_n=64~ and ~repeat_penalty=1.3~
    - [[https://github.com/antimatter15/alpaca.cpp][Alpaca]] is [[https://github.com/tatsu-lab/stanford_alpaca][refined by standford]] for chat instructions
      - [[https://github.com/lm-sys/FastChat][Vicuna]] is refined alpaca with sharegpt with 'conversation format'
    - WizardLM is fine tuned with  'Evol-Instruct' (llm generated) data for 'deep knowledge'
      - VicunaWizard combines the Vicuna and Wizardlm
    - Can be fine tuned with [[https://github.com/tloen/alpaca-lora][LoRA]]
  - Flan-T5 for text summary
  - Donut for document image question answering
  - Vilt for image question answering
*** Codegen
  - [[https://github.com/bigcode-project/starcoder][StarCoder]]
  - [[https://github.com/salesforce/CodeT5][CodeT5+]]
  - [[https://github.com/moyix/fauxpilot][Fauxpilot]]
    - uses salesforce/Codegen which supports natural language input and generation of C, C++, Go, Java, JavaScript, Python (BIG QUERY).
      - specialized in python with the BIGPYTHON dataset
    - Converts salesforce/codegen model into GPTJ
** Speech to text
  - open ai whisper translation
#+NAME: yt-translate
#+BEGIN_SRC sh :tangle no
pip install --user git+https://github.com/openai/whisper.git
pip install --user yt-dlp
VID="TyvE8oexEAA"
yt-dlp https://www.youtube.com/watch?v=${VID} --format m4a -o "%(id)s.%(ext)s"
whisper "/content/${VID}.m4a" --model small --language English
#+END_SRC
** Text to speech
- [[https://github.com/neonbjb/tortoise-tts][tortoise-tts]] based on dalle
- [[https://github.com/coqui-ai/TTS][coqui-ai]] with YourTTS/FreeVC voice cloning
  - [[https://github.com/coqui-ai/TTS/blob/dev/TTS/tts/utils/text/cleaners.py][english cleaner]] for abbrevations/dates/times/numbers
** Agent prompts
#+NAME: chain-of-thought-tools
#+BEGIN_SRC text :tangle no
Answer the following questions as best you can. You have access to the following tools:

{tools}

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Begin! Remember the following: {context}

Question: {input}
{agent_scratchpad}
#+END_SRC
** transformers examples
#+NAME: hugginface.py
#+BEGIN_SRC python :tangle no
#!/usr/bin/env python3
# sentiment analysis
from transformers import pipeline
# from transformers import load_dataset
classifier = pipeline("sentiment-analysis")
print(classifier("ara ara"))

# LLM
import torch
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM
MIN_TRANSFORMERS_VERSION = '4.25.1'
print("checking transformers version")
# check transformers version
assert transformers.__version__ >= MIN_TRANSFORMERS_VERSION, f'Please upgrade transformers to version {MIN_TRANSFORMERS_VERSION} or higher.'
print("Getting tokenizer")
# init
tokenizer = AutoTokenizer.from_pretrained("togethercomputer/RedPajama-INCITE-Chat-3B-v1")
print("getting model")
model = AutoModelForCausalLM.from_pretrained("togethercomputer/RedPajama-INCITE-Chat-3B-v1", torch_dtype=torch.bfloat16) # , device_map='auto', load_in_8bit=True
# infern
print("Feeding prompt")
prompt = "<human>: Where is Jimmy Hoffa?\n<bot>:"
inputs = tokenizer(prompt, return_tensors='pt').to(model.device)
input_length = inputs.input_ids.shape[1]
print("Generating")
outputs = model.generate(
    ,**inputs, max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.7, top_k=50, return_dict_in_generate=True
)
token = outputs.sequences[0, input_length:]
output_str = tokenizer.decode(token)
print(output_str)

# Agents
import torch
from transformers import LocalAgent
model = "bigcode/tiny_starcoder_py"
agent = LocalAgent.from_pretrained(model, torch_dtype=torch.bfloat16)
text = "Sally sold sea shells down by the seashore."
prompt = "Summarize the text given in the variable `text` and read it out loud."
agent.run(prompt, text=text)#return_code=True
#https://huggingface.co/datasets/huggingface-tools/default-prompts
# quant with offload
model = AutoModelForCausalLm.from_pretrained("bigcode/starcoder", device_map="auto", load_in_8bit=True, offload_folder="offload", offload_state_dict=True)
# distribute weights on cpu/gpu
from accelerate import infer_auto_device_map
from accelerate import init_empty_weights
from transformers import GPTBigCodeConfig, GPTBigCodeForCausalLM

device_map = {}
model_config = GPTBigCodeConfig()
with init_empty_weights(): # get device_map without loading model weights
    model = GPTBigCodeForCausalLM(model_config)
    device_map = infer_auto_device_map(model, max_memory={0: "0GiB", "cpu": "24GiB"})

#+END_SRC
